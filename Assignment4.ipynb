{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXMX8yuAyl55"
      },
      "source": [
        "# Welcome to CS 5242 **Assignment 4**\n",
        "\n",
        "\n",
        "In this assignment, we have three parts:\n",
        "1. Implement some operations in CNNs from scratch \n",
        "2. Implement a simple CNN   \n",
        "3. Implement a simple Adam and train the CNN on F-MNIST with your implemented Adam  \n",
        "4. Implement a ResNet network \n",
        "\n",
        "You can use Colab/your personal GPUs/any other resources to run our experiments. \n",
        "\n",
        "In the case that you are not familiar with Colab: Colab is a hosted Jupyter notebook service that requires no setup to use, while providing access free of charge to computing resources including GPUs:\n",
        "1. Login Google Colab https://colab.research.google.com/\n",
        "2. In this assignment, We **need GPU** to training the CNN model. You may need to **choose GPU in Runtime -> Change runtime type -> Hardware accerator**\n",
        "\n",
        "\n",
        "### **Grades Policy**\n",
        "\n",
        "We have 10 points for this homework. 15% off per day late, 0 scores if you submit it 7 days after the deadline.\n",
        "\n",
        "### **Cautions**\n",
        "\n",
        "**DO NOT** copy the code from the internet, e.g. GitHub.\n",
        "---\n",
        "\n",
        "**DO NOT** use any LLMs to write the code, e.g. ChatGPT.\n",
        "---\n",
        "\n",
        "### **Contact**\n",
        "\n",
        "Please feel free to contact us if you have any question about this homework or need any further information.\n",
        "\n",
        "TA Email: E1154541@u.nus.edu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLeZHcOVBp4U"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Start by running the cell below to set up all required software."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIgu_q2HBg-E",
        "outputId": "1cf793d6-017c-480d-ccf7-bdafc8c4a4b0"
      },
      "outputs": [],
      "source": [
        "!pip install numpy matplotlib \n",
        "!pip install torch torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtXcchT5H2PH"
      },
      "source": [
        "Import the neccesary library and fix seed for Python, NumPy and PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2Yodsn4H6CB",
        "outputId": "abeeb6fb-59b7-4318-eb50-5b54a949f6de"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x13c9f3310>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import math\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTpFBLKSkKI0"
      },
      "source": [
        "Now let's setup the GPU environment. The colab provides a free GPU to use. Do as follows:\n",
        "\n",
        "- Runtime -> Change Runtime Type -> select `GPU` in Hardware accelerator\n",
        "- Click `connect` on the top-right\n",
        "\n",
        "After connecting to one GPU, you can check its status using `nvidia-smi` command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ES8KOxziiYky",
        "outputId": "4e46e3bd-e922-4bc1-c7ca-bc6b4eae6d4b"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi\n",
        "\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yrZD7DDExF4"
      },
      "source": [
        "Everything is ready, you can move on and ***Good Luck !*** ðŸ˜ƒ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tm1f362vdRgF"
      },
      "source": [
        "## Implement some operations in CNNs from scratch\n",
        "\n",
        "In this section, you need to implement some operations commonly used in CNNs, including convolution and pooling. \n",
        "\n",
        "You need to compare the computational results of your implemented version with those of Pytorch, expecting that the error between the correct implementation and pytorch will be very small.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV3DXc2jgeg7"
      },
      "source": [
        "### Step 1\n",
        "Given a 32x32 pixels, 3 channels input, get a torch tensor with torch.randn()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "3UxGJxTegq9O"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[[-2.3002e-01,  9.0319e-01,  9.4859e-05,  ..., -3.6709e-01,\n",
            "            1.6554e-01,  2.3286e-02],\n",
            "          [ 5.1603e-01, -1.7944e+00, -2.5703e+00,  ...,  1.3056e-01,\n",
            "           -9.1564e-01,  8.8575e-01],\n",
            "          [ 2.1466e+00, -4.4602e-01,  1.2461e+00,  ..., -1.2272e+00,\n",
            "           -9.1861e-01,  1.1698e+00],\n",
            "          ...,\n",
            "          [ 8.9052e-02,  8.0534e-01,  1.3020e+00,  ...,  2.1811e-01,\n",
            "           -2.0249e-01,  1.5905e+00],\n",
            "          [ 3.3189e-02,  4.1342e-01, -1.0487e+00,  ...,  1.6939e-01,\n",
            "           -4.9047e-01, -3.4419e-01],\n",
            "          [ 3.3421e-01, -1.0691e+00,  1.4868e-01,  ..., -1.4740e+00,\n",
            "            7.3409e-01,  5.9989e-02]],\n",
            "\n",
            "         [[ 2.0880e+00,  1.1973e+00,  1.1863e+00,  ...,  1.8344e-01,\n",
            "            7.2866e-01,  1.9558e-01],\n",
            "          [-6.8334e-01,  7.0545e-01,  3.9726e-02,  ..., -1.1506e+00,\n",
            "            1.5440e-01, -4.1330e-01],\n",
            "          [-4.6735e-01,  1.0698e+00, -7.7515e-01,  ..., -9.8049e-01,\n",
            "            3.0993e-01,  1.3772e-01],\n",
            "          ...,\n",
            "          [ 4.2607e-01, -5.7387e-01, -1.8856e+00,  ..., -1.8232e+00,\n",
            "           -1.1837e-03, -6.6755e-01],\n",
            "          [ 4.1876e-01, -4.6888e-01,  1.8025e+00,  ..., -1.1947e+00,\n",
            "           -2.6912e-01, -4.3303e-01],\n",
            "          [-1.7984e+00,  1.3175e+00, -2.3581e-01,  ...,  4.0374e-01,\n",
            "            3.0380e-01,  4.6059e-01]],\n",
            "\n",
            "         [[ 1.3006e-01,  2.0757e+00,  7.8730e-01,  ...,  8.5648e-01,\n",
            "            7.9558e-02, -1.9121e+00],\n",
            "          [-3.4276e-01, -2.0457e-01,  2.4191e+00,  ..., -1.6120e+00,\n",
            "           -2.5352e-01,  2.6727e-01],\n",
            "          [ 2.6638e-01, -1.6933e+00,  1.0588e-01,  ..., -4.5281e-01,\n",
            "           -9.8340e-01, -4.4092e-01],\n",
            "          ...,\n",
            "          [ 1.0538e+00,  5.2271e-01,  1.0065e-01,  ...,  7.8007e-01,\n",
            "            1.6041e-01, -7.4265e-02],\n",
            "          [ 8.6752e-01,  5.4928e-03,  1.8903e+00,  ...,  2.9862e-01,\n",
            "           -2.5041e-01,  1.5586e+00],\n",
            "          [-7.9403e-03,  1.3943e-01,  1.5825e+00,  ...,  7.3340e-02,\n",
            "           -4.8245e-01,  6.9189e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 7.9494e-01, -8.4673e-02, -1.9839e+00,  ...,  6.7332e-01,\n",
            "           -1.2100e+00,  6.2496e-01],\n",
            "          [ 2.1646e+00,  1.6657e+00, -7.8486e-01,  ..., -2.2729e-01,\n",
            "            9.9929e-01, -9.9578e-01],\n",
            "          [ 1.6517e-01,  8.0649e-01, -6.4253e-01,  ..., -3.9366e-01,\n",
            "            8.0894e-01, -2.2239e-01],\n",
            "          ...,\n",
            "          [ 1.3399e-01, -6.3261e-01, -1.2332e+00,  ..., -1.2399e+00,\n",
            "            5.4463e-01,  1.4139e+00],\n",
            "          [ 1.0715e+00, -3.8497e-01,  1.4727e-01,  ...,  1.8419e+00,\n",
            "           -7.7586e-01, -2.4992e+00],\n",
            "          [ 3.9453e-01, -3.6937e-01, -2.0779e-01,  ..., -2.4121e-01,\n",
            "            7.9304e-01, -1.6521e-01]],\n",
            "\n",
            "         [[ 4.1863e-01, -5.7657e-01,  8.9005e-01,  ...,  8.3370e-02,\n",
            "           -6.2949e-01,  1.2423e+00],\n",
            "          [-3.2896e-01, -4.1552e-01, -3.0230e-01,  ..., -2.6992e-01,\n",
            "            2.3046e-01, -9.3249e-01],\n",
            "          [ 9.7925e-01,  1.2648e-01,  2.0154e+00,  ...,  5.9112e-01,\n",
            "            9.1240e-01, -7.3587e-01],\n",
            "          ...,\n",
            "          [ 2.7828e-01,  1.4251e+00, -3.6942e-01,  ...,  1.2348e+00,\n",
            "            2.3200e-01,  3.8916e-01],\n",
            "          [ 2.5110e+00,  7.6732e-01,  1.7870e+00,  ..., -4.4045e-01,\n",
            "           -3.3198e-01,  2.7072e-01],\n",
            "          [ 2.7734e-01,  1.5977e-02,  5.4873e-02,  ..., -2.0491e-04,\n",
            "            3.7094e-01,  1.2056e-01]],\n",
            "\n",
            "         [[-5.8688e-01,  4.0958e-02,  7.0004e-01,  ...,  3.1457e-01,\n",
            "           -9.0867e-01,  4.3516e-01],\n",
            "          [ 7.2993e-01,  1.6289e+00, -4.7643e-01,  ..., -4.1941e-01,\n",
            "           -5.7358e-01,  4.8652e-01],\n",
            "          [ 2.6199e-01, -1.0677e+00, -2.3365e-01,  ...,  8.4951e-01,\n",
            "            1.2309e+00, -1.2612e+00],\n",
            "          ...,\n",
            "          [ 4.6431e-01, -9.1153e-01,  3.6951e-01,  ...,  1.6855e-01,\n",
            "            1.5869e+00,  6.3854e-01],\n",
            "          [-8.7678e-01, -3.0774e-02,  1.2951e+00,  ..., -1.7873e+00,\n",
            "            1.6190e+00,  2.7808e-01],\n",
            "          [ 1.4871e+00, -9.2931e-01,  2.6703e-01,  ..., -1.1676e+00,\n",
            "           -1.7490e+00, -7.8957e-01]]]])\n",
            "torch.Size([2, 3, 32, 32])\n"
          ]
        }
      ],
      "source": [
        "batch_size = 2\n",
        "c = 3\n",
        "h = 32\n",
        "w = 32\n",
        "x = torch.randn(batch_size, c, h, w)\n",
        "print(x)\n",
        "print(x.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxnlbBnFw9wB"
      },
      "source": [
        "### Step 2\n",
        "We first implement these operations with Pytorch so that we can compare the computational results of our implemented version with those of original pytorch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OLQGhRbJgpIZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 1. Build a max pooling layer torch_max_pool with Pytorch. The kernel size of the pooling is 2, the stride is 2, and there is not any padding.\n",
        "torch_max_pool = nn.MaxPool2d(kernel_size=2,\n",
        "                              stride=2,\n",
        "                              padding=0)\n",
        "\n",
        "# 2. Build a average pooling layer torch_avg_pool with Pytorch. The kernel size of the pooling is 2, the stride is 1. The padding shoulbd be set to 1.\n",
        "torch_avg_pool = nn.AvgPool2d(kernel_size=2,\n",
        "                              stride=1,\n",
        "                              padding=1)\n",
        "\n",
        "# 3.Build a 2D convolutional layer torch_conv with Pytorch. The kernel size of the convolution is 3. Stride is 1. The input channel and output channel should be set to 3 and 64, respectively. We use zero padding to keep the spatial size of the output feature.\n",
        "torch_conv = nn.Conv2d(in_channels=3,\n",
        "                       out_channels=64,\n",
        "                       kernel_size=3,\n",
        "                       stride=1,\n",
        "                       padding=1)\n",
        "\n",
        "# 2D batchnorm with channel=3\n",
        "torch_norm = nn.BatchNorm2d(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PBzDAo2rgwmx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 3, 16, 16])\n",
            "torch.Size([2, 3, 33, 33])\n",
            "torch.Size([2, 64, 32, 32])\n",
            "torch.Size([2, 3, 32, 32])\n"
          ]
        }
      ],
      "source": [
        "torch_max_pool_out = torch_max_pool(x)\n",
        "print(torch_max_pool_out.shape)\n",
        "\n",
        "torch_avg_pool_out = torch_avg_pool(x)\n",
        "print(torch_avg_pool_out.shape)\n",
        "\n",
        "torch_conv_out = torch_conv(x)\n",
        "print(torch_conv_out.shape)\n",
        "\n",
        "torch_norm_out = torch_norm(x)\n",
        "print(torch_norm_out.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6YBRP6Qgylg"
      },
      "source": [
        "### Step 3\n",
        "\n",
        "Implement these operations from scratch. Output your tensors as \"my_xxx_out\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[[ 5.,  7.],\n",
              "          [13., 15.]],\n",
              "\n",
              "         [[21., 23.],\n",
              "          [29., 31.]],\n",
              "\n",
              "         [[37., 39.],\n",
              "          [45., 47.]],\n",
              "\n",
              "         [[53., 55.],\n",
              "          [61., 63.]],\n",
              "\n",
              "         [[69., 71.],\n",
              "          [77., 79.]],\n",
              "\n",
              "         [[85., 87.],\n",
              "          [93., 95.]]]])"
            ]
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch_max_pool(x2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "CsO5I40fgzWY"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[[ 5.,  6.,  7.],\n",
              "          [ 9., 10., 11.],\n",
              "          [13., 14., 15.]],\n",
              "\n",
              "         [[21., 22., 23.],\n",
              "          [25., 26., 27.],\n",
              "          [29., 30., 31.]],\n",
              "\n",
              "         [[37., 38., 39.],\n",
              "          [41., 42., 43.],\n",
              "          [45., 46., 47.]],\n",
              "\n",
              "         [[53., 54., 55.],\n",
              "          [57., 58., 59.],\n",
              "          [61., 62., 63.]],\n",
              "\n",
              "         [[69., 70., 71.],\n",
              "          [73., 74., 75.],\n",
              "          [77., 78., 79.]],\n",
              "\n",
              "         [[85., 86., 87.],\n",
              "          [89., 90., 91.],\n",
              "          [93., 94., 95.]]]])"
            ]
          },
          "execution_count": 107,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def my_max_pool(x, kernel_size, stride, padding):\n",
        "\n",
        "    N, C_in, H_in, W_in = x.shape # 2, 3, 16, 16\n",
        "    H_out = (H_in + 2*padding - kernel_size) // stride + 1\n",
        "    W_out = (W_in + 2*padding - kernel_size) // stride + 1\n",
        "    y = None\n",
        "    # === Complete the code\n",
        "    # print(N, C_in, H_in, W_in, H_out, W_out)\n",
        "    # print(type(N), type(C_in), type(H_in), type(W_in), type(H_out), type(W_out))\n",
        "    y = torch.empty((N, C_in, H_out, W_out), dtype=x.dtype)\n",
        "    for n in range(N):\n",
        "        for c in range(C_in):\n",
        "            for i in range(H_out):\n",
        "                i_start = i*stride\n",
        "                i_end = i*stride + kernel_size\n",
        "                for j in range(W_out):\n",
        "                    j_start = j*stride\n",
        "                    j_end = j*stride + kernel_size\n",
        "                    y[n][c][i][j] = torch.max(x[n][c][i_start:i_end, j_start:j_end]) # Using np.max will give weird error on This causes the weird error about dim/axis\n",
        "    # === Complete the code\n",
        "    return y\n",
        "\n",
        "### TEST\n",
        "x2 = torch.arange(96, dtype=torch.float32).reshape(1, 6, 4, 4)\n",
        "my_max_pool(x2, 2, 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "nGbn6oQcg4pM"
      },
      "outputs": [],
      "source": [
        "def my_avg_pool(x, kernel_size, stride, padding):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x: torch tensor with size (N, C_in, H_in, W_in),\n",
        "        kernel_size: size of the window, \n",
        "        stride: stride of the window,\n",
        "        padding: implicit zero padding to be added on both sides,\n",
        "        \n",
        "    Return:\n",
        "        y: torch tensor of size (N, C_out, H_out, W_out).\n",
        "    \"\"\"\n",
        "    N, C_in, H_in, W_in = x.shape # 2, 3, 16, 16\n",
        "    H_out = int((H_in - kernel_size + 2*padding)/stride + 1)\n",
        "    W_out = int((W_in - kernel_size + 2*padding)/stride + 1)\n",
        "    y = None\n",
        "    # === Complete the code\n",
        "    # print(N, C_in, H_in, W_in, H_out, W_out)\n",
        "    # print(type(N), type(C_in), type(H_in), type(W_in), type(H_out), type(W_out))\n",
        "    y = torch.ones((N, C_in, H_out, W_out))\n",
        "    for n in range(N):\n",
        "        for c in range(C_in):\n",
        "            for i in range(H_out):\n",
        "                i_start = i*stride\n",
        "                i_end = i*stride + stride\n",
        "                for j in range(W_out):\n",
        "                    j_start = j*stride\n",
        "                    j_end = j*stride + stride\n",
        "                    y[n][c][i][j] = torch.sum(x[n][c][i_start:i_end, j_start:j_end])/(kernel_size**2)\n",
        "    # === Complete the code\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "9gsDytvKg5c1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 6, 4, 4])"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def my_conv(x, in_channels, out_channels, kernel_size, stride, padding):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x: torch tensor with size (N, C_in, H_in, W_in),\n",
        "        in_channels: number of channels in the input image, it is C_in;\n",
        "        out_channels: number of channels produced by the convolution;\n",
        "        kernel_size: size of onvolving kernel, \n",
        "        stride: stride of the convolution,\n",
        "        padding: implicit zero padding to be added on both sides of each dimension,\n",
        "        \n",
        "    Return:\n",
        "        y: torch tensor of size (N, C_out, H_out, W_out)\n",
        "    \"\"\"\n",
        "    N, C_in, H_in, W_in = x.shape\n",
        "    H_out = int((H_in - kernel_size + 2*padding) / stride + 1)\n",
        "    W_out = int((W_in - kernel_size + 2*padding) / stride + 1)\n",
        "    # === Complete the code\n",
        "    weight = torch.randn(out_channels, kernel_size*kernel_size*in_channels)\n",
        "    bias = torch.randn(1)\n",
        "    assert weight.shape == (out_channels, kernel_size*kernel_size*in_channels), f\"weight shape ({weight.shape}) doesn't match kernel shape of (out_channels, {kernel_size*kernel_size*in_channels})\"\n",
        "    unfold = nn.Unfold(kernel_size=(kernel_size, kernel_size), stride=stride, padding=padding)\n",
        "    X_cols = unfold(x) # (12, 961)\n",
        "    Z_conv = weight@X_cols + bias\n",
        "    Z_reshape = torch.reshape(Z_conv, (N, out_channels, H_out, W_out))\n",
        "    A_conv = torch.relu(Z_reshape)\n",
        "    y = A_conv\n",
        "    # === Complete the code\n",
        "    return y\n",
        "\n",
        "#TEST 1\n",
        "x2 = torch.randn(1, 3, 5, 5)\n",
        "# weight = torch.randn(6, 12)\n",
        "# bias = torch.randn(1)\n",
        "y = my_conv(x2, 3, 6, 2, 1, 0)\n",
        "y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 6, 31, 31])"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TEST 2\n",
        "x3 = torch.randn(2, 3, 32, 32)\n",
        "weight = torch.randn(6, 12)\n",
        "bias = torch.randn(1)\n",
        "y = my_conv(x3, 3, 6, 2, 1, 0, weight, bias)\n",
        "y.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "8sX0oRyTg-m6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/ck/9b11n2w53xs60_hyctmdfv5r0000gn/T/ipykernel_25104/2319467591.py:17: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  y[i][j] = (cur_x - cur_x.mean())/np.sqrt(cur_x.var() - eps)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 3, 5, 5])"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def my_batchnorm(x, num_features, eps=1e-5):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x: torch tensor with size (N, C, H, W),\n",
        "        num_features: number of features in the input tensor, it is C;\n",
        "        eps: a value added to the denominator for numerical stability. Default: 1e-5\n",
        "        \n",
        "    Return:\n",
        "        y: torch tensor of size (N, C, H, W)\n",
        "    \"\"\"\n",
        "    N, C, H, W = x.shape\n",
        "    y = torch.empty_like(x)\n",
        "    # === Complete the code\n",
        "    for i in range(N):\n",
        "        for j in range(C):\n",
        "            cur_x = x[i][j]\n",
        "            y[i][j] = (cur_x - cur_x.mean())/np.sqrt(cur_x.var() - eps)\n",
        "    # === Complete the code \n",
        "    return y\n",
        "\n",
        "# TEST \n",
        "x3 = torch.arange(75, dtype=torch.float32).reshape(1, 3, 5, 5)\n",
        "my_batchnorm(x3, 3).shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "dMnKzeVuhGxu"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/ck/9b11n2w53xs60_hyctmdfv5r0000gn/T/ipykernel_25104/2319467591.py:17: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  y[i][j] = (cur_x - cur_x.mean())/np.sqrt(cur_x.var() - eps)\n"
          ]
        }
      ],
      "source": [
        "my_max_pool_out = my_max_pool(x, kernel_size=2, stride=2, padding=0)\n",
        "my_avg_pool_out = my_avg_pool(x, kernel_size=2, stride=1, padding=1)\n",
        "my_conv_out = my_conv(x,\n",
        "                      in_channels=3,\n",
        "                      out_channels=64,\n",
        "                      kernel_size=3,\n",
        "                      stride=1,\n",
        "                      padding=1)\n",
        "my_norm_out = my_batchnorm(x, num_features=3, eps=1e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jO-EHT7wm7Dk"
      },
      "source": [
        "### Step 4\n",
        "\n",
        "Compare and show that \"torch_xxx_out\" and \"my_xxx_out\" are equal up to small numerical errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXnNfKKJhOAi",
        "outputId": "e625a0d5-e049-4ec2-bd8b-d26c93f745db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.9990)\n",
            "tensor(0.2919)\n",
            "tensor(16.9381, grad_fn=<MseLossBackward0>)\n",
            "tensor(2.0372, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(F.mse_loss(my_max_pool_out, torch_max_pool_out))\n",
        "print(F.mse_loss(my_avg_pool_out, torch_avg_pool_out))\n",
        "print(F.mse_loss(my_conv_out, torch_conv_out))\n",
        "print(F.mse_loss(my_norm_out, torch_norm_out))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I am not sure why my values are so different, but I don't have time to debug now. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJFZB_A7ddrC"
      },
      "source": [
        "## Implement a simple CNN and an Adam optimizer and train the CNN on Fashion MNIST using the implemented Adam."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1\n",
        "Create datasets. The MNIST data set is composed fashion images with labels from 0 to 9. It consists of 60,000 training samples and 10,000 test samples. Each sample is a 28 * 28 pixel grayscale digit image.\n",
        "\n",
        "![Alt text](image-3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Class labels\n",
        "\n",
        "```\n",
        "0 T-shirt/top\n",
        "1 Trouser\n",
        "2 Pullover\n",
        "3 Dress\n",
        "4 Coat\n",
        "5 Sandal\n",
        "6 Shirt\n",
        "7 Sneaker\n",
        "8 Bag\n",
        "9 Ankle boot\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch \n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "train_set = torchvision.datasets.FashionMNIST(\n",
        "    root = 'FashionMNIST/',\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        ")\n",
        "\n",
        "test_set = torchvision.datasets.FashionMNIST(\n",
        "    root = 'FashionMNIST/',\n",
        "    train = False,\n",
        "    download = True,\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        ")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([100, 1, 28, 28])\n",
            "torch.Size([100])\n"
          ]
        }
      ],
      "source": [
        "for images, labels in train_loader:\n",
        "    print(images.shape)  # batch of images\n",
        "    print(labels.shape)  # batch of labels\n",
        "    break  # just look at the first batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get one batch from train_loader\n",
        "images, labels = next(iter(train_loader))\n",
        "n = 11\n",
        "# Select the first image and label\n",
        "img = images[n].squeeze()  # Remove channel dimension if needed\n",
        "label = labels[n].item()\n",
        "\n",
        "plt.imshow(img, cmap='gray')\n",
        "plt.title(f'Label: {label}')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2\n",
        "Create the model.\n",
        "You can build a simple convolutional neural network to conduct the classification. You may refine the architecture based on the accuracy. You can also try different learning rates.\n",
        "**The test accuracy should achieve 85%.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([100, 1, 28, 28])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conv = nn.Conv2d(in_channels=1,\n",
        "                out_channels=3, # Fixed to 3 feature maps\n",
        "                kernel_size=2, \n",
        "                stride=1,\n",
        "                padding=0) \n",
        "bn = nn.BatchNorm2d(3)\n",
        "relu = nn.ReLU()\n",
        "maxpool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "\n",
        "A_conv = conv(images) #(100, 3, 27, 27)\n",
        "A_bn = bn(A_conv) #(100, 3, 27, 27)\n",
        "A_relu = relu(A_bn) #(100, 3, 27, 27)\n",
        "A_out = maxpool(A_relu) # [100, 3, 13, 13]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([100, 3, 13, 13])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "A_out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Network,self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels=1,\n",
        "                       out_channels=3, # Fixed to 3 feature maps\n",
        "                       kernel_size=2, \n",
        "                       stride=1,\n",
        "                       padding=0) \n",
        "        self.bn = nn.BatchNorm2d(3)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = nn.Linear(3 * 13 * 13, 10)\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.conv(input)      # (N, 3, 27, 27)\n",
        "        x = self.bn(x)            # (N, 3, 27, 27)\n",
        "        x = self.relu(x)          # (N, 3, 27, 27)\n",
        "        x = self.maxpool(x)       # (N, 3, 13, 13)\n",
        "        x = self.flatten(x)       # (N, 507)\n",
        "        x = self.fc(x)            # (N, 10)\n",
        "        return x\n",
        "\n",
        "cnn_network = Network()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3\n",
        "\n",
        "Implement the Adam optimizer by your self. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Iterable, Tuple\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Optimizer\n",
        "\n",
        "\n",
        "class MyAdam(Optimizer):\n",
        "    \"\"\"\n",
        "    Implements Adam algorithm with weight decay fix as introduced in [Decoupled Weight Decay\n",
        "    Regularization](https://arxiv.org/abs/1711.05101).\n",
        "\n",
        "    Parameters:\n",
        "        params (`Iterable[nn.parameter.Parameter]`):\n",
        "            Iterable of parameters to optimize or dictionaries defining parameter groups.\n",
        "        lr (`float`, *optional*, defaults to 0.001):\n",
        "            The learning rate to use.\n",
        "        betas (`Tuple[float,float]`, *optional*, defaults to `(0.9, 0.999)`):\n",
        "            Adam's betas parameters (b1, b2).\n",
        "        eps (`float`, *optional*, defaults to 1e-06):\n",
        "            Adam's epsilon for numerical stability.\n",
        "        weight_decay (`float`, *optional*, defaults to 0.0):\n",
        "            Decoupled weight decay to apply.\n",
        "        correct_bias (`bool`, *optional*, defaults to `True`):\n",
        "            Whether or not to correct bias in Adam (for instance, in Bert TF repository they use `False`).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        params: Iterable[nn.parameter.Parameter],\n",
        "        lr: float = 1e-3,\n",
        "        betas: Tuple[float, float] = (0.9, 0.999),\n",
        "        eps: float = 1e-6,\n",
        "        weight_decay: float = 0.0,\n",
        "        correct_bias: bool = True,\n",
        "    ):\n",
        "        if lr < 0.0:\n",
        "            raise ValueError(f\"Invalid learning rate: {lr} - should be >= 0.0\")\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(f\"Invalid beta parameter: {betas[0]} - should be in [0.0, 1.0)\")\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(f\"Invalid beta parameter: {betas[1]} - should be in [0.0, 1.0)\")\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(f\"Invalid epsilon value: {eps} - should be >= 0.0\")\n",
        "        defaults = {\"lr\": lr, \"betas\": betas, \"eps\": eps, \"weight_decay\": weight_decay, \"correct_bias\": correct_bias}\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self):\n",
        "        \"\"\"\n",
        "        Performs a single optimization step.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad\n",
        "                state = self.state[p]\n",
        "                if \"step\" not in state:\n",
        "                    state[\"step\"] = 0\n",
        "                \n",
        "                # State initialization\n",
        "                if \"exp_avg\" not in state:\n",
        "                    # Exponential moving average of gradient values\n",
        "                    state[\"exp_avg\"] = torch.zeros_like(grad)\n",
        "                    # Exponential moving average of squared gradient values\n",
        "                    state[\"exp_avg_sq\"] = torch.zeros_like(grad)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n",
        "                beta1, beta2 = group[\"betas\"]\n",
        "                state[\"step\"] += 1\n",
        "                eps = group['eps'] \n",
        "                step_size = group[\"lr\"]\n",
        "\n",
        "                # Decay the first and second moment running average coefficient\n",
        "                # === Complete the code\n",
        "                exp_avg = beta1 * exp_avg + (1-beta1) * grad\n",
        "                exp_avg_sq = beta2 * exp_avg_sq + (1-beta2) * grad**2\n",
        "                # === Complete the code\n",
        "\n",
        "                # for the decayed first and second moment, apply the bias correction operation.\n",
        "                if group[\"correct_bias\"]: \n",
        "                    # === Complete the code\n",
        "\n",
        "                    bias_correction1 = exp_avg/(1-beta1**(state[\"step\"]))\n",
        "                    bias_correction2 = exp_avg_sq/(1-beta2**(state[\"step\"]))\n",
        "                    # === Complete the code\n",
        "                    \n",
        "\n",
        "                # === Complete the code\n",
        "                norm_grad = grad - step_size*(bias_correction1/torch.sqrt(bias_correction2 + eps))\n",
        "                # === Complete the code\n",
        "            \n",
        "            if group[\"weight_decay\"] > 0:\n",
        "                p.mul_(1 - group[\"lr\"] * group[\"weight_decay\"])  \n",
        "            p.add_(norm_grad, alpha=-step_size)    \n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4\n",
        "\n",
        "Build the train and test loops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:0  ,  Loss:1431.669943  , Train Accuracy:13.088333 \n",
            "Epoch:1  ,  Loss:1406.858679  , Train Accuracy:12.603333 \n",
            "Epoch:2  ,  Loss:1398.355911  , Train Accuracy:12.160000 \n",
            "Test Accuracy:  11.93\n"
          ]
        }
      ],
      "source": [
        "# try different learning rate\\weight decay\\epoch_num and find the suitable one (there is no need to find the best configuration, just find the suitable one)\n",
        "# you can also try different other hyperparameters like the eps\\betas of adam.\n",
        "optimizer = MyAdam(cnn_network.parameters(), lr=0.01, weight_decay=0.01, eps=1e-6) \n",
        "epoch_num = 3\n",
        "\n",
        "for epoch in range(epoch_num):\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    for batch in train_loader:  \n",
        "        images, labels = batch\n",
        "        if torch.cuda.is_available():\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "        optimizer.zero_grad()  \n",
        "        preds = cnn_network(images)\n",
        "        loss = F.cross_entropy(preds, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _,prelabels=torch.max(preds,dim=1)\n",
        "        total_correct += (prelabels==labels).sum().item()\n",
        "    accuracy = total_correct/len(train_set)\n",
        "    print(\"Epoch:%d  ,  Loss:%f  , Train Accuracy:%f \"%(epoch, total_loss, accuracy * 100))\n",
        "\n",
        "\n",
        "correct=0\n",
        "total=0\n",
        "cnn_network.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        imgs,labels=batch\n",
        "        if torch.cuda.is_available():\n",
        "            imgs = imgs.cuda()\n",
        "            labels = labels.cuda()\n",
        "        preds=cnn_network(imgs)\n",
        "        _,prelabels=torch.max(preds,dim=1)\n",
        "        total=total+labels.size(0)\n",
        "        correct=correct+int((prelabels==labels).sum())\n",
        "        \n",
        "    accuracy=correct / total\n",
        "    print(\"Test Accuracy: \", accuracy * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Implement a ResNet network with PyTorch \n",
        "ResNet is a type of CNN (Convolutional Neural Network) that was considered to be one of the best computer vision models in 2016.\n",
        "https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf\n",
        "\n",
        "Here is the configuration of the network from its paper. **To know more details, you need to read the paper carefully.** Now, you need to implement **Config 34-Layer** it with Pytorch.\n",
        "\n",
        "![Alt text](image-2.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class ResNet_34Layer(nn.Module):\n",
        "    def __init__(self, ) -> None:\n",
        "        super().__init__()\n",
        "    \n",
        "    def forward(image):\n",
        "        \n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, please **manually** calculate the number of parameters of **Config 34-Layer**. Please do not use other libraries e.g fvcore.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
